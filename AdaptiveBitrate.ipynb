{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4oC09i8ym5FzmB3miEDkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsjeon87/DeepReinforcementLearning/blob/master/AdaptiveBitrate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#구글 drive mount 방법\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn_kvbXbpaBa",
        "outputId": "a181198e-51ad-497b-edc6-96678020da66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Munch\n",
        "!pip install yacs\n",
        "!pip install -U transformers==4.45.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If4j3del46qH",
        "outputId": "45ebf5d2-7387-4743-f19e-5980e1272710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Munch\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: Munch\n",
            "Successfully installed Munch-4.0.0\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs) (6.0.2)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n",
            "Collecting transformers==4.45.2\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (0.5.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.2) (2025.1.31)\n",
            "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.2\n",
            "    Uninstalling transformers-4.50.2:\n",
            "      Successfully uninstalled transformers-4.50.2\n",
            "Successfully installed tokenizers-0.20.3 transformers-4.45.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tflean 에러 발생 -> !pip install tflearn"
      ],
      "metadata": {
        "id": "qFhg_psdqovF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "experience data 수집\n",
        "\n",
        "# command examples:\n",
        "    # python generate_exp_pool.py --models mpc --traces fcc-train --video video1 --trace-num -1 --seed 1 --fixed-order --cuda-id 0\n"
      ],
      "metadata": {
        "id": "rCei8tDEqEy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/generate_exp_pool.py --models mpc --trace fcc-train --video video1 --trace-num -1 --seed 1 --fixed-order --cuda-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHZqiOgTphCd",
        "outputId": "d5995e43-fc72-40eb-b2b9-a31c7ea77ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-25 03:33:16.387019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742873596.407168    2876 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742873596.413299    2876 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-25 03:33:16.433473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(models=['mpc'], trace='fcc-train', video='video1', trace_num=-1, seed=1, cuda_id=0, fixed_order=True)\n",
            "/content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/traces/train/fcc-train/\n",
            "Loading traces from /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/traces/train/fcc-train/\n",
            "235\n",
            "Collect experience with model mpc\n",
            "Done! 0.33898939967551406\n",
            "Done. Experience pool saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/artifacts/exp_pools/fcc-train_video1/mpc/seed_1_trace_num_-1_fixed_True/exp_pool.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    # >>> for debug <<<\n",
        "    # args.exp_pool_path = 'artifacts/exp_pools/exp_pool.pkl'\n",
        "    # args.plm_type = 'llama'\n",
        "    # args.plm_size = 'base'\n",
        "    # args.rank = 128\n",
        "    # args.state_feature_dim = 256\n",
        "    # args.num_epochs = 1\n",
        "    # args.eval_per_epoch = 1\n",
        "    # args.adapt = True\n",
        "    # args.test = True\n",
        "    # args.device = 'cuda:0'\n",
        "    # args.device_out = 'cuda:0'\n",
        "    # args.which_layer = -1\n",
        "    # args.seed = 100003\n",
        "    # >>> for debug <<<"
      ],
      "metadata": {
        "id": "MJlJKoLbx1B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "python run_plm.py --adapt --test --grad-accum-steps 32 --seed 666 --plm-type llama --plm-size base --rank 128 --device cuda:0 --state-feature-dim 256 --w 20 --gamma 1. --lr 0.0001 --warmup-steps 2000 --num-epochs 80 --eval-per-epoch 2 --target-return-scale 1"
      ],
      "metadata": {
        "id": "PdeAlRNt3chE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/run_plm.py --adapt --test --grad-accum-steps 32 --seed 666 --plm-type llama --plm-size base --rank 128 --device cuda:0 --state-feature-dim 256 --w 20 --gamma 1. --lr 0.0001 --warmup-steps 2000 --num-epochs 80 --eval-per-epoch 2 --target-return-scale 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMX6lUfc4n2x",
        "outputId": "57c7fe52-7f33-4ec7-9bdf-b08e7a32375b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments:\n",
            "Namespace(exp_pool_path='/content/drive/MyDrive/netllm/adaptive_bitrate_streaming/artifacts/exp_pools/exp_pool.pkl', sample_step=None, trace='fcc-test', trace_num=100, video='video1', fixed_order=False, plm_type='llama', plm_size='base', rank=128, state_feature_dim=256, w=20, gamma=1.0, lr=0.0001, weight_decay=0.0001, warmup_steps=2000, num_epochs=80, eval_per_epoch=2, save_checkpoint_per_epoch=2, target_return_scale=1.0, which_layer=-1, adapt=True, test=True, grad_accum_steps=32, seed=666, scale=1000, model_dir=None, device='cuda:0', device_out='cuda:0', device_mid=None)\n",
            "Loading traces from /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/traces/test/fcc-test/\n",
            "Experience dataset info:\n",
            "Munch({'max_reward': np.float64(4.3), 'min_reward': np.float64(-85.0127377757031), 'max_return': 0.04661987504979122, 'min_return': 0.0006304750495579298, 'min_timestep': 0, 'max_timestep': 46, 'max_action': 5, 'min_action': 0})\n",
            "If tokenizer is loaded:  [128000, 15339, 1917] \n",
            "\n",
            "pad token is None, set to id 128256\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
            "Step 0 - mean train loss  1.592824\n",
            "Step 100 - mean train loss  2.251608\n",
            "Step 200 - mean train loss  2.220850\n",
            "Step 300 - mean train loss  2.237713\n",
            "Step 400 - mean train loss  2.236506\n",
            "Step 500 - mean train loss  2.243770\n",
            "Step 600 - mean train loss  2.243577\n",
            "Step 700 - mean train loss  2.241314\n",
            "Step 800 - mean train loss  2.236447\n",
            "Step 900 - mean train loss  2.235941\n",
            "==================== Training Iteration #0 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 369.9196422100067,\n",
            " 'training/train_loss_mean': np.float64(2.2326220115983344),\n",
            " 'training/train_loss_std': np.float64(0.31827709744579286)}\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/0\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(3.6041921432351693),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(3.6041921432351693),\n",
            " 'time/evaluation': 678.396155834198}\n",
            "Step 0 - mean train loss  2.454861\n",
            "Step 100 - mean train loss  2.229750\n",
            "Step 200 - mean train loss  2.208940\n",
            "Step 300 - mean train loss  2.191214\n",
            "Step 400 - mean train loss  2.193045\n",
            "Step 500 - mean train loss  2.181681\n",
            "Step 600 - mean train loss  2.181554\n",
            "Step 700 - mean train loss  2.183876\n",
            "Step 800 - mean train loss  2.186425\n",
            "Step 900 - mean train loss  2.176003\n",
            "==================== Training Iteration #1 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 394.34721755981445,\n",
            " 'training/train_loss_mean': np.float64(2.1694361237158257),\n",
            " 'training/train_loss_std': np.float64(0.3203037212560861)}\n",
            "Step 0 - mean train loss  2.808278\n",
            "Step 100 - mean train loss  2.133616\n",
            "Step 200 - mean train loss  2.111260\n",
            "Step 300 - mean train loss  2.080150\n",
            "Step 400 - mean train loss  2.072481\n",
            "Step 500 - mean train loss  2.068450\n",
            "Step 600 - mean train loss  2.062959\n",
            "Step 700 - mean train loss  2.064794\n",
            "Step 800 - mean train loss  2.061913\n",
            "Step 900 - mean train loss  2.061703\n",
            "==================== Training Iteration #2 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 394.4139726161957,\n",
            " 'training/train_loss_mean': np.float64(2.0583860326004793),\n",
            " 'training/train_loss_std': np.float64(0.3337190589682479)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/2\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(3.818155759245401),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(3.818155759245401),\n",
            " 'time/evaluation': 678.9418525695801}\n",
            "Step 0 - mean train loss  2.098340\n",
            "Step 100 - mean train loss  1.986949\n",
            "Step 200 - mean train loss  1.987525\n",
            "Step 300 - mean train loss  1.970103\n",
            "Step 400 - mean train loss  1.960210\n",
            "Step 500 - mean train loss  1.942898\n",
            "Step 600 - mean train loss  1.937468\n",
            "Step 700 - mean train loss  1.941391\n",
            "Step 800 - mean train loss  1.945184\n",
            "Step 900 - mean train loss  1.931728\n",
            "==================== Training Iteration #3 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 394.2319276332855,\n",
            " 'training/train_loss_mean': np.float64(1.9272485083365536),\n",
            " 'training/train_loss_std': np.float64(0.3785494797060857)}\n",
            "Step 0 - mean train loss  2.494369\n",
            "Step 100 - mean train loss  1.925005\n",
            "Step 200 - mean train loss  1.881617\n",
            "Step 300 - mean train loss  1.844874\n",
            "Step 400 - mean train loss  1.831415\n",
            "Step 500 - mean train loss  1.823087\n",
            "Step 600 - mean train loss  1.819017\n",
            "Step 700 - mean train loss  1.818485\n",
            "Step 800 - mean train loss  1.814046\n",
            "Step 900 - mean train loss  1.817851\n",
            "==================== Training Iteration #4 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 393.3281626701355,\n",
            " 'training/train_loss_mean': np.float64(1.8141513263365352),\n",
            " 'training/train_loss_std': np.float64(0.4323697545674583)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/4\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(4.053883075930908),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(4.053883075930908),\n",
            " 'time/evaluation': 678.7527148723602}\n",
            "Step 0 - mean train loss  1.679406\n",
            "Step 100 - mean train loss  1.740039\n",
            "Step 200 - mean train loss  1.772494\n",
            "Step 300 - mean train loss  1.758731\n",
            "Step 400 - mean train loss  1.738547\n",
            "Step 500 - mean train loss  1.718846\n",
            "Step 600 - mean train loss  1.712161\n",
            "Step 700 - mean train loss  1.720267\n",
            "Step 800 - mean train loss  1.727327\n",
            "Step 900 - mean train loss  1.713039\n",
            "==================== Training Iteration #5 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 393.6865713596344,\n",
            " 'training/train_loss_mean': np.float64(1.7136825807242988),\n",
            " 'training/train_loss_std': np.float64(0.5015007511008791)}\n",
            "Step 0 - mean train loss  2.124192\n",
            "Step 100 - mean train loss  1.754810\n",
            "Step 200 - mean train loss  1.697749\n",
            "Step 300 - mean train loss  1.659199\n",
            "Step 400 - mean train loss  1.646258\n",
            "Step 500 - mean train loss  1.637231\n",
            "Step 600 - mean train loss  1.636613\n",
            "Step 700 - mean train loss  1.635400\n",
            "Step 800 - mean train loss  1.629460\n",
            "Step 900 - mean train loss  1.637343\n",
            "==================== Training Iteration #6 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 393.58303022384644,\n",
            " 'training/train_loss_mean': np.float64(1.6343123528313446),\n",
            " 'training/train_loss_std': np.float64(0.5387669097755101)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/6\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(4.192462471329974),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(4.192462471329974),\n",
            " 'time/evaluation': 679.165540933609}\n",
            "Step 0 - mean train loss  1.381867\n",
            "Step 100 - mean train loss  1.572819\n",
            "Step 200 - mean train loss  1.629158\n",
            "Step 300 - mean train loss  1.616533\n",
            "Step 400 - mean train loss  1.588273\n",
            "Step 500 - mean train loss  1.568286\n",
            "Step 600 - mean train loss  1.562010\n",
            "Step 700 - mean train loss  1.571774\n",
            "Step 800 - mean train loss  1.579440\n",
            "Step 900 - mean train loss  1.564349\n",
            "==================== Training Iteration #7 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 395.3346645832062,\n",
            " 'training/train_loss_mean': np.float64(1.5700328738754055),\n",
            " 'training/train_loss_std': np.float64(0.592549421244781)}\n",
            "Step 0 - mean train loss  1.788205\n",
            "Step 100 - mean train loss  1.631110\n",
            "Step 200 - mean train loss  1.569199\n",
            "Step 300 - mean train loss  1.531650\n",
            "Step 400 - mean train loss  1.522707\n",
            "Step 500 - mean train loss  1.516627\n",
            "Step 600 - mean train loss  1.520225\n",
            "Step 700 - mean train loss  1.519885\n",
            "Step 800 - mean train loss  1.511986\n",
            "Step 900 - mean train loss  1.522500\n",
            "==================== Training Iteration #8 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 394.13846158981323,\n",
            " 'training/train_loss_mean': np.float64(1.5203124120173206),\n",
            " 'training/train_loss_std': np.float64(0.605718805728486)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/8\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(4.243336332788797),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(4.243336332788797),\n",
            " 'time/evaluation': 679.0108270645142}\n",
            "Step 0 - mean train loss  1.201881\n",
            "Step 100 - mean train loss  1.472852\n",
            "Step 200 - mean train loss  1.544254\n",
            "Step 300 - mean train loss  1.532333\n",
            "Step 400 - mean train loss  1.500062\n",
            "Step 500 - mean train loss  1.481529\n",
            "Step 600 - mean train loss  1.476839\n",
            "Step 700 - mean train loss  1.486437\n",
            "Step 800 - mean train loss  1.492666\n",
            "Step 900 - mean train loss  1.477145\n",
            "==================== Training Iteration #9 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 395.0378031730652,\n",
            " 'training/train_loss_mean': np.float64(1.4866040385511505),\n",
            " 'training/train_loss_std': np.float64(0.6433376196982945)}\n",
            "Step 0 - mean train loss  1.565328\n",
            "Step 100 - mean train loss  1.553066\n",
            "Step 200 - mean train loss  1.493308\n",
            "Step 300 - mean train loss  1.459170\n",
            "Step 400 - mean train loss  1.452723\n",
            "Step 500 - mean train loss  1.449989\n",
            "Step 600 - mean train loss  1.456555\n",
            "Step 700 - mean train loss  1.457717\n",
            "Step 800 - mean train loss  1.447747\n",
            "Step 900 - mean train loss  1.459365\n",
            "==================== Training Iteration #10 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 393.52641105651855,\n",
            " 'training/train_loss_mean': np.float64(1.4576125474998272),\n",
            " 'training/train_loss_std': np.float64(0.6392255794258594)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/10\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(4.2659094043412455),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(4.2659094043412455),\n",
            " 'time/evaluation': 680.2384150028229}\n",
            "Step 0 - mean train loss  1.082609\n",
            "Step 100 - mean train loss  1.418368\n",
            "Step 200 - mean train loss  1.494712\n",
            "Step 300 - mean train loss  1.483885\n",
            "Step 400 - mean train loss  1.451585\n",
            "Step 500 - mean train loss  1.434626\n",
            "Step 600 - mean train loss  1.431673\n",
            "Step 700 - mean train loss  1.440056\n",
            "Step 800 - mean train loss  1.443897\n",
            "Step 900 - mean train loss  1.428132\n",
            "==================== Training Iteration #11 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 394.432697057724,\n",
            " 'training/train_loss_mean': np.float64(1.43912186696527),\n",
            " 'training/train_loss_std': np.float64(0.6605318736755756)}\n",
            "Step 0 - mean train loss  1.435131\n",
            "Step 100 - mean train loss  1.501173\n",
            "Step 200 - mean train loss  1.447003\n",
            "Step 300 - mean train loss  1.416183\n",
            "Step 400 - mean train loss  1.409671\n",
            "Step 500 - mean train loss  1.408987\n",
            "Step 600 - mean train loss  1.416681\n",
            "Step 700 - mean train loss  1.419161\n",
            "Step 800 - mean train loss  1.407948\n",
            "Step 900 - mean train loss  1.419991\n",
            "==================== Training Iteration #12 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 392.5723354816437,\n",
            " 'training/train_loss_mean': np.float64(1.4185397825596562),\n",
            " 'training/train_loss_std': np.float64(0.6463849002538943)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/12\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(4.277678367082767),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(4.277678367082767),\n",
            " 'time/evaluation': 678.3253707885742}\n",
            "Step 0 - mean train loss  1.007362\n",
            "Step 100 - mean train loss  1.384455\n",
            "Step 200 - mean train loss  1.458773\n",
            "Step 300 - mean train loss  1.449299\n",
            "Step 400 - mean train loss  1.418604\n",
            "Step 500 - mean train loss  1.402750\n",
            "Step 600 - mean train loss  1.401247\n",
            "Step 700 - mean train loss  1.408463\n",
            "Step 800 - mean train loss  1.410153\n",
            "Step 900 - mean train loss  1.394376\n",
            "==================== Training Iteration #13 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 393.2905328273773,\n",
            " 'training/train_loss_mean': np.float64(1.405594642367588),\n",
            " 'training/train_loss_std': np.float64(0.6575465329455988)}\n",
            "Step 0 - mean train loss  1.357513\n",
            "Step 100 - mean train loss  1.461384\n",
            "Step 200 - mean train loss  1.413136\n",
            "Step 300 - mean train loss  1.384704\n",
            "Step 400 - mean train loss  1.376781\n",
            "Step 500 - mean train loss  1.377328\n",
            "Step 600 - mean train loss  1.385294\n",
            "Step 700 - mean train loss  1.388902\n",
            "Step 800 - mean train loss  1.377325\n",
            "Step 900 - mean train loss  1.389882\n",
            "==================== Training Iteration #14 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 392.81442761421204,\n",
            " 'training/train_loss_mean': np.float64(1.3887526190663917),\n",
            " 'training/train_loss_std': np.float64(0.6412238088464288)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/14\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(4.29293464672877),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(4.29293464672877),\n",
            " 'time/evaluation': 678.6698453426361}\n",
            "Step 0 - mean train loss  0.972776\n",
            "Step 100 - mean train loss  1.359628\n",
            "Step 200 - mean train loss  1.429393\n",
            "Step 300 - mean train loss  1.421466\n",
            "Step 400 - mean train loss  1.392789\n",
            "Step 500 - mean train loss  1.377699\n",
            "Step 600 - mean train loss  1.377362\n",
            "Step 700 - mean train loss  1.383654\n",
            "Step 800 - mean train loss  1.383616\n",
            "Step 900 - mean train loss  1.367823\n",
            "==================== Training Iteration #15 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 394.2565369606018,\n",
            " 'training/train_loss_mean': np.float64(1.3787001737180244),\n",
            " 'training/train_loss_std': np.float64(0.647757414387794)}\n",
            "Step 0 - mean train loss  1.307142\n",
            "Step 100 - mean train loss  1.428920\n",
            "Step 200 - mean train loss  1.385958\n",
            "Step 300 - mean train loss  1.359432\n",
            "Step 400 - mean train loss  1.349459\n",
            "Step 500 - mean train loss  1.350829\n",
            "Step 600 - mean train loss  1.358740\n",
            "Step 700 - mean train loss  1.363311\n",
            "Step 800 - mean train loss  1.351759\n",
            "Step 900 - mean train loss  1.364959\n",
            "==================== Training Iteration #16 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 391.7600145339966,\n",
            " 'training/train_loss_mean': np.float64(1.3640319597439834),\n",
            " 'training/train_loss_std': np.float64(0.6340370593672797)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/16\n",
            "Best model saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            ">>>>>>>>>> Evaluation Information\n",
            "{'best_return': np.float64(4.311177187805562),\n",
            " 'episodes_len': 4700,\n",
            " 'episodes_return': np.float64(4.311177187805562),\n",
            " 'time/evaluation': 677.5052909851074}\n",
            "Step 0 - mean train loss  0.971151\n",
            "Step 100 - mean train loss  1.339870\n",
            "Step 200 - mean train loss  1.404806\n",
            "Step 300 - mean train loss  1.398515\n",
            "Step 400 - mean train loss  1.371912\n",
            "Step 500 - mean train loss  1.357245\n",
            "Step 600 - mean train loss  1.357716\n",
            "Step 700 - mean train loss  1.363163\n",
            "Step 800 - mean train loss  1.361700\n",
            "Step 900 - mean train loss  1.345765\n",
            "==================== Training Iteration #17 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 391.5631814002991,\n",
            " 'training/train_loss_mean': np.float64(1.3560210195889912),\n",
            " 'training/train_loss_std': np.float64(0.6374167471621264)}\n",
            "Step 0 - mean train loss  1.271115\n",
            "Step 100 - mean train loss  1.402028\n",
            "Step 200 - mean train loss  1.363203\n",
            "Step 300 - mean train loss  1.338553\n",
            "Step 400 - mean train loss  1.325834\n",
            "Step 500 - mean train loss  1.327622\n",
            "Step 600 - mean train loss  1.335289\n",
            "Step 700 - mean train loss  1.340707\n",
            "Step 800 - mean train loss  1.329284\n",
            "Step 900 - mean train loss  1.343159\n",
            "==================== Training Iteration #18 ====================\n",
            ">>>>>>>>>> Training Information:\n",
            "{'time/training': 392.3320655822754,\n",
            " 'training/train_loss_mean': np.float64(1.3422616236959117),\n",
            " 'training/train_loss_std': np.float64(0.6286541709663782)}\n",
            "Checkpoint saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_checkpoint/18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oeLUDsVdqbXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6agTj3Tgwo2",
        "outputId": "5f8137e7-cde2-4d02-82e9-88e00ef8a341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.49.0\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/run_plm.py --test --grad-accum-steps 32 --seed 666 --plm-type llama --plm-size base --rank 128 --device cuda:0 --state-feature-dim 256 --w 20 --gamma 1. --lr 0.0001 --warmup-steps 2000 --num-epochs 80 --eval-per-epoch 2 --target-return-scale 1"
      ],
      "metadata": {
        "id": "VcFZBL_tgxL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe87b73-0043-4118-d416-355375653a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arguments:\n",
            "Namespace(exp_pool_path='/content/drive/MyDrive/netllm/adaptive_bitrate_streaming/artifacts/exp_pools/exp_pool.pkl', sample_step=None, trace='fcc-test', trace_num=100, video='video1', fixed_order=False, plm_type='llama', plm_size='base', rank=128, state_feature_dim=256, w=20, gamma=1.0, lr=0.0001, weight_decay=0.0001, warmup_steps=2000, num_epochs=80, eval_per_epoch=2, save_checkpoint_per_epoch=2, target_return_scale=1.0, which_layer=-1, adapt=False, test=True, grad_accum_steps=32, seed=666, scale=1000, model_dir=None, device='cuda:0', device_out='cuda:0', device_mid=None)\n",
            "Loading traces from /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/traces/test/fcc-test/\n",
            "Experience dataset info:\n",
            "Munch({'max_reward': np.float64(4.3), 'min_reward': np.float64(-85.0127377757031), 'max_return': 0.04661987504979122, 'min_return': 0.0006304750495579298, 'min_timestep': 0, 'max_timestep': 46, 'max_action': 5, 'min_action': 0})\n",
            "If tokenizer is loaded:  [128000, 15339, 1917] \n",
            "\n",
            "pad token is None, set to id 128256\n",
            "Load model from: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/data/ft_plms/llama_base/adaptive_bitrate_streaming_artifacts_exp_pools_ss_None/rank_128_w_20_gamma_1.0_sfd_256_lr_0.0001_wd_0.0001_warm_2000_epochs_80_seed_666/early_stop_-1_best_model\n",
            "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
            "100\n",
            "{'time': 663.1321051120758, 'mean_reward': np.float64(-3.347581665422251)}\n",
            "Test time: 663.1321051120758 \n",
            "Mean reward: -3.347581665422251\n",
            "Results saved at: /content/drive/MyDrive/netllm/adaptive_bitrate_streaming/artifacts/results/fcc-test_video1/trace_num_100_fixed_True/llama_base/early_stop_-1_rank_128_w_20_gamma_1.0_tgt_scale_1.0_seed_666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFHigORK44Xk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}